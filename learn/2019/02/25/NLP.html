<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>NLP学习 | Xiaoyun Li</title> <meta name="author" content="Xiaoyun R Li"/> <meta name="description" content="Xiaoyun Li's blog "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://humanlee1011.github.io/learn/2019/02/25/NLP.html"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Xiaoyun Li</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">NLP学习</h1> <p class="post-meta">February 25, 2019• Leexy</p> <p class="post-tags"> <a href="/blog/2019"> <i class="fas fa-calendar fa-sm"></i> 2019 </a>   ·   <a href="/blog/tag/%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB"> <i class="fas fa-hashtag fa-sm"></i> 经验分享</a>     ·   <a href="/blog/category/learn"> <i class="fas fa-tag fa-sm"></i> learn</a>   </p> </header> <article class="post-content"> <h1 id="神经网络">神经网络</h1> <h2 id="梯度下降">梯度下降</h2> <p>使用误差方程来计算预测值和真实值的差距，不断调整神经网络中连接的权重。</p> <p>主要是通过梯度下降的方法来找到局部最优的权重值。</p> <h2 id="补充">补充</h2> <h3 id="one-hot编码">one-hot编码</h3> <ol> <li>为什么要binarize分类特征？使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。</li> <li>Why do we embed the feature vectors in the Euclidean space?是因为，在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在<font color="#dd0000">欧式空间的相似度计算，计算余弦相似性</font>，基于的就是欧式空间。</li> <li>将离散型特征进行one-hot编码的作用，是为了让距离计算更合理，但如果特征是离散的，并且不用one-hot编码就可以很合理的计算出距离，那么就没必要进行one-hot编码，比如，该离散特征共有1000个取值，我们分成两组，分别是400和600,两个小组之间的距离有合适的定义，组内的距离也有合适的定义，那就没必要用one-hot 编码。</li> </ol> <h2 id="word2vec">word2vec</h2> <p>在此之前，nlp是将字词转为one-hot编码类型的词向量。缺点在于，数据稀疏性非常高，维度很多，容易造成维度灾难。存在语义鸿沟，无法体现词与词之间的关系。</p> <p>神经网络的训练是有监督的学习，因此要给定输入值和输出值来训练神经网络，而我们最终要获得的是隐藏层的权重矩阵。因为隐藏层的输出事实上是每个输入单词的 “嵌入词向量”。计算的时候，并不会进行矩阵乘法，而是直接找1所在的地方，直接读取词向量。 <img src="https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180719103717765-785211295.png" alt="image"></p> <h2 id="rnn">RNN</h2> <p>是受到人的思考方式的启发，当人看到一个句子时，对单词的理解是基于对上一个单词的认知，因此这个序列关系/上下文关系对单词的理解很有帮助。 RNN是一个具有有限loop的神经网络，如下图所示，$x_t$为输入，$h_t$为输出，$A$是RNN网络结构。 <img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png" width="20%" height="20%" halign="center"></p> <p>将上面这个loop展开后可以看到： <img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" alt="unrolled"></p> <p>RNN的优点：</p> <ol> <li>chain-like的网络结构可以让lists和sequences之间的关系得到学习。</li> <li>可应用于以下方面：speech recognition, language modeling, translation, image captioning… The list goes on.</li> </ol> <h2 id="lstm">LSTM</h2> <p>LSTM是一种特殊的RNN，大多数运用RNN成功的例子都是基于LSTM完成的。 RNN能够通过上下文的关系来预测，但是如果上下文距离很遥远，RNN是无法预测的，比如“I grew up in France… I speak fluent <em>French</em>.”。此处<em>French</em>需要France的relevant information，但是两个单词位于不同的句子中，距离非常遥远。</p> <ul> <li>为什么RNN无法预测？ 在理论上，RNN可以通过configure参数来扩大读取的面积，但是在实际中，RNN并不能handle这种预测，因为depth过深，时间上是无法完成。</li> </ul> <p>相较于RNN，LSTM不同的是在网络结构中，有多种interacting way <img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" alt="LSTM"></p> <h3 id="lstm的core-idea">LSTM的core idea</h3> <p>LSTM具有三种gates，通过sigmoid函数[0,1]来可计算component通过的概率，以此来protect和control这些cell state</p> <p>具体步骤：</p> <ol> <li>forget gate layer，用一个$f_t$来forget掉我们希望忘记的previous subject的info<img src="https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180724213408875-497357476.png" alt="input"> </li> <li>input gate layer，用于决定我们要更新==哪一个值==，创建一个new candidate values$C_t$介入state。<img src="https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180724213929491-1697948414.png" alt="input"> </li> <li>output gate layer, Then we add $i_t * C̃<em>t$，然后和前面已遗忘的$C</em>{t-1}$state相加，$C_t = f_t * C_{t-1} + i_t * C̃_t$<img src="https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180724214355115-204684729.png" alt="input"> </li> </ol> <p>接下来看神经元内最上方的$C{t−1}$.与$a{t−1}$类似，Ct−1也携带着上文的信息。进入神经元后，Ct−1首先会与遗忘权重逐元素相乘，可以想见，由于遗忘权重中值得特点，因此与该权重相乘之后Ct−1​ 中绝大部分的值会变的非常接近0或者非常接近该位置上原来的值。这非常像一扇门，它会决定让哪些Ct−1的元素通过以及通过的比例有多大。反映到实际中，就是对Ct−1中携带的信息进行选择性的遗忘（乘以非常接近0的数）和通过（乘以非常接近1的数），亦即乘以一个权重。</p> <p>理解了遗忘门的作用之后，其他两个门也就比较好理解了。输入门则是对输入信息进行限制，而输入信息就是RNN中的前向运算的结果。经过输入门处理后的信息就可以添加到经过遗忘门处理的上文信息中去，这就是神经元内唯一一个逐元素相加的工作。</p> <h3 id="lstm的variants">LSTM的variants</h3> <ol> <li>将$C_{t-1}$拼接到$h_{t-1}, x_t$后面。</li> </ol> <h2 id="attention机制">Attention机制</h2> <p>注意力模型，这个说到底还只是一个资源分配模型，在某个特定时刻，你的注意力总是集中在画面中的某个焦点部分，而对其它部分视而不见。</p> <h3 id="encoder-decoder框架">Encoder-Decoder框架</h3> <p>Encoder-Decoder模型中的<strong>编码</strong>，就是将输入序列转化成一个固定长度的向量；<strong>解码</strong>，就是将之前生成的固定向量再转化成输出序列。 <img src="https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180725165549037-254698930.png" alt="Encoder-Decoder"></p> <p>缺点：在编码时，后输入的语义信息会覆盖前面输入的语义信息，导致Encoder得到的编码C是有损信息。因而，在解码时，解析的数据本身就不够准确。</p> <p>为了解决上面的弊端，就需要用到Attention Model。在编码时，让code <code class="language-plaintext highlighter-rouge">C</code>拥有一个注意力范围。</p> <h3 id="注意力分配概率分布值">注意力分配概率分布值</h3> <p><img src="https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180725173624266-1186701597.png" alt=""> 可以采用</p> <p>\(c_i = \sum _{j=1}^{T_x}{\alpha_{ij}h_{j}}\) 其中： \(\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{T_x}{exp(e_{ik})}}\) \(e_{ij} = a(s_{i-1}, h_j)\)</p> <p>在上面公式中 $h_j$是Encoder层的隐层第j时刻的输出，$s_{i−1}$是Decoder层第$i−1$时刻隐层的输出。可以发现在计算 $c_i$的模型实际上是一个线性模型，而且$c_i$事实上是Encoder层中各时刻隐层的输出的加权平均值。</p> <p>因此引入Attention 机制，在机器翻译中，模型会自己去学得在不同时刻不同的权重系数$a_{ij}$</p> <h2 id="bi-lstm">Bi-LSTM</h2> <h2 id="参考">参考</h2> <ol> <li>http://colah.github.io/posts/2015-08-Understanding-LSTMs/</li> <li>https://www.cnblogs.com/jiangxinyang/p/9362922.html</li> <li>https://www.cnblogs.com/jiangxinyang/p/9362922.html</li> <li>https://www.cnblogs.com/jiangxinyang/p/9367497.html</li> </ol> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Xiaoyun R Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://github.com/humanlee1011" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>